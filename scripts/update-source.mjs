#!/usr/bin/env zx
// @ts-check
import { $, echo, fs } from 'zx'
import got from 'got'
import { CookieJar } from 'tough-cookie'
import { parseDocument } from 'htmlparser2'
import { findAll } from 'domutils'
import { convert } from 'html-to-text'

const WORK_URL = 'https://archiveofourown.org/works/777002'
const LOCAL_FILE = 'scripts/source.txt'

const cookieJar = new CookieJar()
const http = got.extend({
  cookieJar, // Remember Cloudflare cookies
  retry: { limit: 3 },
  timeout: { request: 30_000 },
  headers: { 'accept-language': 'en-US,en;q=0.7' },
})

/**
 * @param {string} workUrl
 * @returns {Promise<URL>}
 */
async function getDownloadLink(workUrl) {
  const html = await http.get(workUrl, { headers: { accept: 'text/html' } }).text()
  const document = parseDocument(html, { decodeEntities: false, lowerCaseTags: true })
  const actionsEl = findAll((el) => el.name === 'ul' && /\bactions\b/.test(el.attribs['class'] ?? ''), document)
  const linkEls = findAll((el) => el.name === 'a' && 'href' in el.attribs, actionsEl)
  for (const linkEl of linkEls) {
    const { href } = linkEl.attribs
    const match = /\/downloads\/\d+\/[^/?#]+?\.html/i.exec(href)
    if (match) {
      return new URL(href, workUrl)
    }
  }
  throw `Download link not found on ${workUrl}`
}

const localLastUpdatedStr = (await $`git log -1 --pretty="format:%at" -- ${LOCAL_FILE}`) || '0'
const localLastUpdated = new Date(+localLastUpdatedStr * 1000)
echo('Local version was updated at', localLastUpdated.toISOString())

echo('Fetching work page for last updated time')
const downloadLink = await getDownloadLink(WORK_URL)
const updatedAtParam = downloadLink.searchParams.get('updated_at')
if (!updatedAtParam?.match(/^\d+$/)) throw `Missing 'updated_at' in download url: ${downloadLink}`
const remoteLastUpdated = new Date(+updatedAtParam * 1000)
echo('Remote version was updated at', remoteLastUpdated.toISOString())

if (localLastUpdated >= remoteLastUpdated) {
  echo('Up to date')
  process.exit(0)
}

echo('Downloading full HTML from remote')
const html = await http.get(downloadLink, { headers: { accept: 'text/html', referer: WORK_URL } }).text()

echo('Converting HTML to plain text')
const text = convert(html, {
  baseElements: { selectors: ['h1', '.byline', '.userstuff'], orderBy: 'occurrence' },
  wordwrap: 100,
  selectors: [
    { selector: 'a', options: { ignoreHref: true } },
    { selector: 'img', format: 'skip' },
    { selector: 'hr', options: { length: 3 } },
    { selector: 'h1', options: { uppercase: false } },
    { selector: 'h2', options: { uppercase: false } },
    { selector: 'h3', options: { uppercase: false } },
    { selector: 'h4', options: { uppercase: false } },
    { selector: 'h5', options: { uppercase: false } },
    { selector: 'h6', options: { uppercase: false } },
  ],
})

await fs.writeFile(
  LOCAL_FILE,
  `# Autogenerated by scripts/update-source.mjs - DO NOT EDIT!
# Content retrieved from ${WORK_URL}

${text}\n`,
)
const stat = await fs.stat(LOCAL_FILE)
echo('Done,', (stat.size / 1024 / 1204).toFixed(2), 'MB saved to', LOCAL_FILE)
